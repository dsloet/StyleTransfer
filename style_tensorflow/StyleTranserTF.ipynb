{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw7AKY5ZryhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0f5ec100-0aaa-448e-bae2-2c1a1769e3b6"
      },
      "source": [
        "!git clone https://github.com/dsloet/StyleTransfer.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'StyleTransfer'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/38)\u001b[K\rremote: Counting objects:   5% (2/38)\u001b[K\rremote: Counting objects:   7% (3/38)\u001b[K\rremote: Counting objects:  10% (4/38)\u001b[K\rremote: Counting objects:  13% (5/38)\u001b[K\rremote: Counting objects:  15% (6/38)\u001b[K\rremote: Counting objects:  18% (7/38)\u001b[K\rremote: Counting objects:  21% (8/38)\u001b[K\rremote: Counting objects:  23% (9/38)\u001b[K\rremote: Counting objects:  26% (10/38)\u001b[K\rremote: Counting objects:  28% (11/38)\u001b[K\rremote: Counting objects:  31% (12/38)\u001b[K\rremote: Counting objects:  34% (13/38)\u001b[K\rremote: Counting objects:  36% (14/38)\u001b[K\rremote: Counting objects:  39% (15/38)\u001b[K\rremote: Counting objects:  42% (16/38)\u001b[K\rremote: Counting objects:  44% (17/38)\u001b[K\rremote: Counting objects:  47% (18/38)\u001b[K\rremote: Counting objects:  50% (19/38)\u001b[K\rremote: Counting objects:  52% (20/38)\u001b[K\rremote: Counting objects:  55% (21/38)\u001b[K\rremote: Counting objects:  57% (22/38)\u001b[K\rremote: Counting objects:  60% (23/38)\u001b[K\rremote: Counting objects:  63% (24/38)\u001b[K\rremote: Counting objects:  65% (25/38)\u001b[K\rremote: Counting objects:  68% (26/38)\u001b[K\rremote: Counting objects:  71% (27/38)\u001b[K\rremote: Counting objects:  73% (28/38)\u001b[K\rremote: Counting objects:  76% (29/38)\u001b[K\rremote: Counting objects:  78% (30/38)\u001b[K\rremote: Counting objects:  81% (31/38)\u001b[K\rremote: Counting objects:  84% (32/38)\u001b[K\rremote: Counting objects:  86% (33/38)\u001b[K\rremote: Counting objects:  89% (34/38)\u001b[K\rremote: Counting objects:  92% (35/38)\u001b[K\rremote: Counting objects:  94% (36/38)\u001b[K\rremote: Counting objects:  97% (37/38)\u001b[K\rremote: Counting objects: 100% (38/38)\u001b[K\rremote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/34)\u001b[K\rremote: Compressing objects:   5% (2/34)\u001b[K\rremote: Compressing objects:   8% (3/34)\u001b[K\rremote: Compressing objects:  11% (4/34)\u001b[K\rremote: Compressing objects:  14% (5/34)\u001b[K\rremote: Compressing objects:  17% (6/34)\u001b[K\rremote: Compressing objects:  20% (7/34)\u001b[K\rremote: Compressing objects:  23% (8/34)\u001b[K\rremote: Compressing objects:  26% (9/34)\u001b[K\rremote: Compressing objects:  29% (10/34)\u001b[K\rremote: Compressing objects:  32% (11/34)\u001b[K\rremote: Compressing objects:  35% (12/34)\u001b[K\rremote: Compressing objects:  38% (13/34)\u001b[K\rremote: Compressing objects:  41% (14/34)\u001b[K\rremote: Compressing objects:  44% (15/34)\u001b[K\rremote: Compressing objects:  47% (16/34)\u001b[K\rremote: Compressing objects:  50% (17/34)\u001b[K\rremote: Compressing objects:  52% (18/34)\u001b[K\rremote: Compressing objects:  55% (19/34)\u001b[K\rremote: Compressing objects:  58% (20/34)\u001b[K\rremote: Compressing objects:  61% (21/34)\u001b[K\rremote: Compressing objects:  64% (22/34)\u001b[K\rremote: Compressing objects:  67% (23/34)\u001b[K\rremote: Compressing objects:  70% (24/34)\u001b[K\rremote: Compressing objects:  73% (25/34)\u001b[K\rremote: Compressing objects:  76% (26/34)\u001b[K\rremote: Compressing objects:  79% (27/34)\u001b[K\rremote: Compressing objects:  82% (28/34)\u001b[K\rremote: Compressing objects:  85% (29/34)\u001b[K\rremote: Compressing objects:  88% (30/34)\u001b[K\rremote: Compressing objects:  91% (31/34)\u001b[K\rremote: Compressing objects:  94% (32/34)\u001b[K\rremote: Compressing objects:  97% (33/34)\u001b[K\rremote: Compressing objects: 100% (34/34)\u001b[K\rremote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "Unpacking objects:   2% (1/38)   \rUnpacking objects:   5% (2/38)   \rUnpacking objects:   7% (3/38)   \rUnpacking objects:  10% (4/38)   \rUnpacking objects:  13% (5/38)   \rUnpacking objects:  15% (6/38)   \rUnpacking objects:  18% (7/38)   \rUnpacking objects:  21% (8/38)   \rUnpacking objects:  23% (9/38)   \rUnpacking objects:  26% (10/38)   \rUnpacking objects:  28% (11/38)   \rUnpacking objects:  31% (12/38)   \rUnpacking objects:  34% (13/38)   \rUnpacking objects:  36% (14/38)   \rUnpacking objects:  39% (15/38)   \rUnpacking objects:  42% (16/38)   \rUnpacking objects:  44% (17/38)   \rremote: Total 38 (delta 10), reused 19 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  47% (18/38)   \rUnpacking objects:  50% (19/38)   \rUnpacking objects:  52% (20/38)   \rUnpacking objects:  55% (21/38)   \rUnpacking objects:  57% (22/38)   \rUnpacking objects:  60% (23/38)   \rUnpacking objects:  63% (24/38)   \rUnpacking objects:  65% (25/38)   \rUnpacking objects:  68% (26/38)   \rUnpacking objects:  71% (27/38)   \rUnpacking objects:  73% (28/38)   \rUnpacking objects:  76% (29/38)   \rUnpacking objects:  78% (30/38)   \rUnpacking objects:  81% (31/38)   \rUnpacking objects:  84% (32/38)   \rUnpacking objects:  86% (33/38)   \rUnpacking objects:  89% (34/38)   \rUnpacking objects:  92% (35/38)   \rUnpacking objects:  94% (36/38)   \rUnpacking objects:  97% (37/38)   \rUnpacking objects: 100% (38/38)   \rUnpacking objects: 100% (38/38), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3CDuCQrr7wE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae18d525-19c2-4fe0-c36a-41716bd605cb"
      },
      "source": [
        "%cd StyleTransfer/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/StyleTransfer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmSsO2GdsA3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir style_tensorflow/output_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UxOLJD2tK8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f8869844-8ced-4368-c656-8a7c721c573a"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
              " <http.client.HTTPMessage at 0x7fe1a091d320>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gbZ8WmNsH2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4fa05161-693f-44ac-fbe4-69f50f4abedf"
      },
      "source": [
        "%tensorflow_version 1x"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `1x`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxMUmkpQsQ5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from style_tensorflow.StyleTransferTF import get_content_loss, gram_matrix, get_style_loss, get_feature_representations\n",
        "from style_tensorflow.StyleTransferTF import compute_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2G0D0NksuwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from style_tensorflow.utils_tf import load_img, deprocess_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-mIwIfYs3_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c344a469-7b5b-472b-df30-716bf503cec2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing import image as kp_image\n",
        "\n",
        "# Keras is only used to load VGG19 model as a high level API to TensorFlow \n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "# pillow is used for loading and saving images\n",
        "from PIL import Image\n",
        "\n",
        "# numPy is used for manipulation of array of object i.e Image in our case\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3NPAfqYs9j9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of layers to be considered for calculation of Content and Style Loss\n",
        "content_layers = ['block3_conv3']\n",
        "style_layers   = ['block1_conv1','block2_conv2','block4_conv3']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers   = len(style_layers)\n",
        "\n",
        "# path where the content and style images are located\n",
        "content_path = 'images/dancing.jpg'\n",
        "style_path   = 'images/picasso.jpg'\n",
        "\n",
        "# Save the result as\n",
        "save_name = 'dance_picasso.jpg'\n",
        "\n",
        "# path to where Vgg19 model weight is located \n",
        "vgg_weights = \"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHGy7HpktZfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Keras Load VGG19 model\n",
        "def get_model(content_layers,style_layers):\n",
        "\n",
        "  # Load our model. We load pretrained VGG, trained on imagenet data\n",
        "  vgg19           = VGG19(weights=None, include_top=False)\n",
        "\n",
        "  # We don't need to (or want to) train any layers of our pre-trained vgg model, so we set it's trainable to false.\n",
        "  vgg19.trainable = False\n",
        "\n",
        "  style_model_outputs   =  [vgg19.get_layer(name).output for name in style_layers]\n",
        "  content_model_outputs =  [vgg19.get_layer(name).output for name in content_layers]\n",
        "  \n",
        "  model_outputs = content_model_outputs + style_model_outputs\n",
        "\n",
        "  # Build model \n",
        "  return Model(inputs = vgg19.input, outputs = model_outputs),  vgg19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNUuVvbttde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_style_transfer(content_path, style_path, num_iterations=200, content_weight=0.1, style_weight=0.9): \n",
        "\n",
        "  # Create a tensorflow session \n",
        "  sess = tf.Session()\n",
        "\n",
        "  # Assign keras back-end to the TF session which we created\n",
        "  K.set_session(sess)\n",
        "\n",
        "  model, vgg19 = get_model(content_layers,style_layers)\n",
        "\n",
        "  # Get the style and content feature representations (from our specified intermediate layers) \n",
        "  style_features, content_features = get_feature_representations(model, content_path, style_path, num_content_layers)\n",
        "  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "  # VGG default normalization\n",
        "  norm_means = np.array([103.939, 116.779, 123.68])\n",
        "  min_vals = -norm_means\n",
        "  max_vals = 255 - norm_means \n",
        "    \n",
        "\n",
        "  # In original paper, the initial stylized image is random matrix of same size as that of content image\n",
        "  # but in later images content image was used instead on random values for first stylized image\n",
        "  # because it proved to help to stylize faster\n",
        "  generated_image = load_img(content_path)\n",
        "  # generated_image = np.random.randint(0,255, size=generated_image.shape) \n",
        "  \n",
        "  # Create tensorflow variable to hold a stylized/generated image during the training \n",
        "  generated_image = tf.Variable(generated_image, dtype=tf.float32)\n",
        "\n",
        "  model_outputs = model(generated_image)\n",
        "\n",
        "  # weightages of each content and style images i.e alpha & beta\n",
        "  loss_weights = (style_weight, content_weight)\n",
        "\n",
        "  # Create our optimizer\n",
        "  loss = compute_loss(model, loss_weights, model_outputs, gram_style_features, content_features, num_content_layers, num_style_layers)\n",
        "  opt = tf.train.AdamOptimizer(learning_rate=9, beta1=0.9, epsilon=1e-1).minimize( loss[0], var_list = [generated_image])\n",
        "\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(generated_image.initializer)\n",
        "  \n",
        "  # loading the weights again because tf.global_variables_initializer() resets the weights\n",
        "  vgg19.load_weights(vgg_weights)\n",
        "\n",
        "\n",
        "  # Put loss as infinity before training starts and Create a variable to hold best image (i.e image with minimum loss)\n",
        "  best_loss, best_img = float('inf'), None\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "\n",
        "    # Do optimization\n",
        "    sess.run(opt)\n",
        "\n",
        "    # Make sure image values stays in the range of max-min value of VGG norm \n",
        "    clipped = tf.clip_by_value(generated_image, min_vals, max_vals)\n",
        "    # assign the clipped value to the tensor stylized image\n",
        "    generated_image.assign(clipped)\n",
        "\n",
        "\n",
        "    # Open the Tuple of tensors \n",
        "    total_loss, style_score, content_score = loss\n",
        "    total_loss = total_loss.eval(session=sess)\n",
        "\n",
        "\n",
        "    if total_loss < best_loss:\n",
        "\n",
        "      # Update best loss and best image from total loss. \n",
        "      best_loss = total_loss\n",
        "\n",
        "      # generated image is of shape (1, h, w, 3) convert it to (h, w, 3)\n",
        "      temp_generated_image = sess.run(generated_image)[0]\n",
        "      best_img = deprocess_img(temp_generated_image)\n",
        "\n",
        "      s_loss = sess.run(style_score)\n",
        "      c_loss = sess.run(content_score)\n",
        "\n",
        "      # print best loss\n",
        "      print('best: iteration: ', i ,'loss: ', total_loss ,'  style_loss: ',  s_loss,'  content_loss: ', c_loss)\n",
        "\n",
        "    # Save image after every 2 iterations \n",
        "    if (i+1)%10 == 0:\n",
        "      output = Image.fromarray(best_img)\n",
        "      output.save('style_tensorflow/output_images/'+str(i+1)+'-'+save_name)\n",
        "\n",
        "  # after num_iterations iterations are completed, close the TF session \n",
        "  sess.close()\n",
        "      \n",
        "  return best_img, best_loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa1rgmVWt9yQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5d0216c-286f-455f-dac6-173a07401e56"
      },
      "source": [
        "best, best_loss = run_style_transfer(content_path, style_path, num_iterations=200, content_weight=0.1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "best: iteration:  0 loss:  35113504.0   style_loss:  39013160.0   content_loss:  16622.215\n",
            "best: iteration:  1 loss:  31993560.0   style_loss:  35546330.0   content_loss:  18658.658\n",
            "best: iteration:  2 loss:  21476932.0   style_loss:  23861366.0   content_loss:  17031.695\n",
            "best: iteration:  3 loss:  16614883.0   style_loss:  18459168.0   content_loss:  16323.768\n",
            "best: iteration:  4 loss:  13318336.0   style_loss:  14796210.0   content_loss:  17456.623\n",
            "best: iteration:  5 loss:  10285772.0   style_loss:  11426581.0   content_loss:  18504.496\n",
            "best: iteration:  6 loss:  8041367.0   style_loss:  8932766.0   content_loss:  18781.05\n",
            "best: iteration:  7 loss:  6631909.5   style_loss:  7366650.5   content_loss:  19243.39\n",
            "best: iteration:  8 loss:  4929805.5   style_loss:  5475359.0   content_loss:  19823.004\n",
            "best: iteration:  9 loss:  3906299.0   style_loss:  4338063.0   content_loss:  20425.3\n",
            "best: iteration:  10 loss:  3079191.8   style_loss:  3419010.0   content_loss:  20829.076\n",
            "best: iteration:  11 loss:  2585556.5   style_loss:  2870480.5   content_loss:  21240.668\n",
            "best: iteration:  12 loss:  2110517.5   style_loss:  2342612.5   content_loss:  21662.416\n",
            "best: iteration:  13 loss:  1895097.5   style_loss:  2103211.0   content_loss:  22075.83\n",
            "best: iteration:  14 loss:  1741830.4   style_loss:  1932880.0   content_loss:  22383.824\n",
            "best: iteration:  15 loss:  1706954.4   style_loss:  1894101.6   content_loss:  22629.275\n",
            "best: iteration:  16 loss:  1640194.6   style_loss:  1819903.0   content_loss:  22819.857\n",
            "best: iteration:  17 loss:  1637531.0   style_loss:  1816925.0   content_loss:  22985.133\n",
            "best: iteration:  18 loss:  1597916.4   style_loss:  1772897.0   content_loss:  23091.463\n",
            "best: iteration:  19 loss:  1566787.4   style_loss:  1738297.4   content_loss:  23197.252\n",
            "best: iteration:  20 loss:  1472435.5   style_loss:  1633446.6   content_loss:  23337.84\n",
            "best: iteration:  21 loss:  1372066.1   style_loss:  1521905.6   content_loss:  23509.736\n",
            "best: iteration:  22 loss:  1234003.9   style_loss:  1368484.2   content_loss:  23681.246\n",
            "best: iteration:  23 loss:  1103707.6   style_loss:  1223691.4   content_loss:  23854.195\n",
            "best: iteration:  24 loss:  956847.75   style_loss:  1060496.8   content_loss:  24007.014\n",
            "best: iteration:  25 loss:  828372.94   style_loss:  917736.2   content_loss:  24104.004\n",
            "best: iteration:  26 loss:  702715.7   style_loss:  778113.8   content_loss:  24132.879\n",
            "best: iteration:  27 loss:  606819.9   style_loss:  671560.0   content_loss:  24158.443\n",
            "best: iteration:  28 loss:  523979.34   style_loss:  579507.94   content_loss:  24221.627\n",
            "best: iteration:  29 loss:  469860.56   style_loss:  519367.4   content_loss:  24299.523\n",
            "best: iteration:  30 loss:  426753.44   style_loss:  471465.06   content_loss:  24349.203\n",
            "best: iteration:  31 loss:  403317.25   style_loss:  445420.3   content_loss:  24389.666\n",
            "best: iteration:  32 loss:  384645.12   style_loss:  424667.44   content_loss:  24444.508\n",
            "best: iteration:  33 loss:  377043.7   style_loss:  416214.8   content_loss:  24503.336\n",
            "best: iteration:  34 loss:  367949.1   style_loss:  406104.62   content_loss:  24549.695\n",
            "best: iteration:  35 loss:  361646.5   style_loss:  399095.25   content_loss:  24607.77\n",
            "best: iteration:  36 loss:  350357.12   style_loss:  386542.66   content_loss:  24687.236\n",
            "best: iteration:  37 loss:  339256.7   style_loss:  374200.5   content_loss:  24762.652\n",
            "best: iteration:  38 loss:  324523.53   style_loss:  357823.62   content_loss:  24822.732\n",
            "best: iteration:  39 loss:  308927.5   style_loss:  340487.06   content_loss:  24891.707\n",
            "best: iteration:  40 loss:  291630.97   style_loss:  321260.38   content_loss:  24966.377\n",
            "best: iteration:  41 loss:  273694.75   style_loss:  301325.0   content_loss:  25022.537\n",
            "best: iteration:  42 loss:  256921.48   style_loss:  282682.47   content_loss:  25072.346\n",
            "best: iteration:  43 loss:  240240.33   style_loss:  264140.25   content_loss:  25141.053\n",
            "best: iteration:  44 loss:  226342.06   style_loss:  248689.78   content_loss:  25212.441\n",
            "best: iteration:  45 loss:  212857.58   style_loss:  233701.42   content_loss:  25262.973\n",
            "best: iteration:  46 loss:  202309.3   style_loss:  221975.3   content_loss:  25315.309\n",
            "best: iteration:  47 loss:  192665.5   style_loss:  211252.03   content_loss:  25386.557\n",
            "best: iteration:  48 loss:  184983.4   style_loss:  202708.83   content_loss:  25454.908\n",
            "best: iteration:  49 loss:  178436.97   style_loss:  195428.86   content_loss:  25510.047\n",
            "best: iteration:  50 loss:  172559.81   style_loss:  188892.53   content_loss:  25565.39\n",
            "best: iteration:  51 loss:  167888.69   style_loss:  183696.92   content_loss:  25614.652\n",
            "best: iteration:  52 loss:  163020.03   style_loss:  178283.7   content_loss:  25646.834\n",
            "best: iteration:  53 loss:  158857.78   style_loss:  173655.44   content_loss:  25678.97\n",
            "best: iteration:  54 loss:  154506.83   style_loss:  168816.58   content_loss:  25718.865\n",
            "best: iteration:  55 loss:  150219.8   style_loss:  164049.64   content_loss:  25751.475\n",
            "best: iteration:  56 loss:  146154.06   style_loss:  159529.22   content_loss:  25777.668\n",
            "best: iteration:  57 loss:  141827.48   style_loss:  154718.77   content_loss:  25805.814\n",
            "best: iteration:  58 loss:  137948.67   style_loss:  150406.9   content_loss:  25824.502\n",
            "best: iteration:  59 loss:  134122.52   style_loss:  146154.8   content_loss:  25832.043\n",
            "best: iteration:  60 loss:  130468.1   style_loss:  142092.56   content_loss:  25847.99\n",
            "best: iteration:  61 loss:  127251.36   style_loss:  138516.28   content_loss:  25867.12\n",
            "best: iteration:  62 loss:  124088.625   style_loss:  135001.3   content_loss:  25874.61\n",
            "best: iteration:  63 loss:  121235.37   style_loss:  131830.39   content_loss:  25880.277\n",
            "best: iteration:  64 loss:  118601.516   style_loss:  128902.87   content_loss:  25889.396\n",
            "best: iteration:  65 loss:  116003.06   style_loss:  126014.95   content_loss:  25896.078\n",
            "best: iteration:  66 loss:  113666.54   style_loss:  123417.516   content_loss:  25907.84\n",
            "best: iteration:  67 loss:  111402.43   style_loss:  120899.74   content_loss:  25926.648\n",
            "best: iteration:  68 loss:  109184.42   style_loss:  118433.664   content_loss:  25941.207\n",
            "best: iteration:  69 loss:  107151.98   style_loss:  116173.766   content_loss:  25955.99\n",
            "best: iteration:  70 loss:  105159.19   style_loss:  113957.67   content_loss:  25972.898\n",
            "best: iteration:  71 loss:  103210.56   style_loss:  111791.484   content_loss:  25982.238\n",
            "best: iteration:  72 loss:  101392.75   style_loss:  109770.66   content_loss:  25991.668\n",
            "best: iteration:  73 loss:  99592.26   style_loss:  107768.234   content_loss:  26008.498\n",
            "best: iteration:  74 loss:  97831.65   style_loss:  105810.32   content_loss:  26023.64\n",
            "best: iteration:  75 loss:  96170.23   style_loss:  103962.695   content_loss:  26037.965\n",
            "best: iteration:  76 loss:  94535.91   style_loss:  102144.95   content_loss:  26054.56\n",
            "best: iteration:  77 loss:  92933.75   style_loss:  100363.43   content_loss:  26066.63\n",
            "best: iteration:  78 loss:  91419.97   style_loss:  98680.29   content_loss:  26077.184\n",
            "best: iteration:  79 loss:  89947.7   style_loss:  97043.16   content_loss:  26088.73\n",
            "best: iteration:  80 loss:  88504.58   style_loss:  95438.72   content_loss:  26097.383\n",
            "best: iteration:  81 loss:  87132.305   style_loss:  93912.67   content_loss:  26108.992\n",
            "best: iteration:  82 loss:  85812.914   style_loss:  92445.19   content_loss:  26122.469\n",
            "best: iteration:  83 loss:  84513.26   style_loss:  91000.26   content_loss:  26130.246\n",
            "best: iteration:  84 loss:  83258.234   style_loss:  89604.99   content_loss:  26137.443\n",
            "best: iteration:  85 loss:  82054.0   style_loss:  88266.18   content_loss:  26144.348\n",
            "best: iteration:  86 loss:  80873.67   style_loss:  86954.19   content_loss:  26149.064\n",
            "best: iteration:  87 loss:  79713.375   style_loss:  85664.03   content_loss:  26157.537\n",
            "best: iteration:  88 loss:  78591.03   style_loss:  84415.95   content_loss:  26166.71\n",
            "best: iteration:  89 loss:  77498.914   style_loss:  83201.64   content_loss:  26174.395\n",
            "best: iteration:  90 loss:  76422.125   style_loss:  82004.37   content_loss:  26181.973\n",
            "best: iteration:  91 loss:  75367.414   style_loss:  80832.06   content_loss:  26185.629\n",
            "best: iteration:  92 loss:  74344.305   style_loss:  79694.76   content_loss:  26190.203\n",
            "best: iteration:  93 loss:  73345.66   style_loss:  78584.28   content_loss:  26198.059\n",
            "best: iteration:  94 loss:  72365.22   style_loss:  77494.13   content_loss:  26204.967\n",
            "best: iteration:  95 loss:  71407.57   style_loss:  76429.125   content_loss:  26213.615\n",
            "best: iteration:  96 loss:  70477.53   style_loss:  75394.93   content_loss:  26220.938\n",
            "best: iteration:  97 loss:  69571.54   style_loss:  74387.71   content_loss:  26226.006\n",
            "best: iteration:  98 loss:  68682.92   style_loss:  73399.62   content_loss:  26232.578\n",
            "best: iteration:  99 loss:  67811.484   style_loss:  72430.65   content_loss:  26239.023\n",
            "best: iteration:  100 loss:  66960.43   style_loss:  71484.05   content_loss:  26247.803\n",
            "best: iteration:  101 loss:  66129.47   style_loss:  70559.83   content_loss:  26256.246\n",
            "best: iteration:  102 loss:  65315.066   style_loss:  69654.31   content_loss:  26261.875\n",
            "best: iteration:  103 loss:  64515.387   style_loss:  68765.06   content_loss:  26268.332\n",
            "best: iteration:  104 loss:  63730.73   style_loss:  67892.65   content_loss:  26273.469\n",
            "best: iteration:  105 loss:  62962.67   style_loss:  67038.61   content_loss:  26279.2\n",
            "best: iteration:  106 loss:  62211.26   style_loss:  66203.08   content_loss:  26284.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-80fd936a29d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_style_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-3be6aa0a3b76>\u001b[0m in \u001b[0;36mrun_style_transfer\u001b[0;34m(content_path, style_path, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Make sure image values stays in the range of max-min value of VGG norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mclipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;31m# assign the clipped value to the tensor stylized image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mgenerated_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py\u001b[0m in \u001b[0;36mclip_by_value\u001b[0;34m(t, clip_value_min, clip_value_max, name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Go through list of tensors, for each value in each tensor clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mt_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Assert that the shape is compatible with the initial shape,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# to prevent unintentional broadcasting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mminimum\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6554\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6555\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6556\u001b[0;31m         \"Minimum\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6557\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6558\u001b[0m     result = _dispatch.dispatch(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    529\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    269\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[1;32m    270\u001b[0m              \"dtype\": dtype_value},\n\u001b[0;32m--> 271\u001b[0;31m       name=name).outputs[0]\n\u001b[0m\u001b[1;32m    272\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3355\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input #%d is not a tensor: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m     return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\n\u001b[0;32m-> 3357\u001b[0;31m                                     attrs, op_def, compute_device)\n\u001b[0m\u001b[1;32m   3358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m   def _create_op_internal(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3424\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3426\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3427\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1777\u001b[0m     output_types = [\n\u001b[1;32m   1778\u001b[0m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOutputType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m     ]\n\u001b[1;32m   1781\u001b[0m     self._outputs = [\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1777\u001b[0m     output_types = [\n\u001b[1;32m   1778\u001b[0m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOutputType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m     ]\n\u001b[1;32m   1781\u001b[0m     self._outputs = [\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/c_api_util.py\u001b[0m in \u001b[0;36mtf_output\u001b[0;34m(c_op, index)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mWrapped\u001b[0m \u001b[0mTF_Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \"\"\"\n\u001b[0;32m--> 186\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_TF_Output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_Output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m     \u001b[0m__getattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_Output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"oper\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Output_oper_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m_swig_getattr\u001b[0;34m(self, class_type, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swig_getattr_nondynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRbYSrphuCAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}